{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# spacy Interal Data structures.\n\n#all shared data is stored into vocab\n# words , tags, and their hashid\n\n# string store is mapping of word to its hashid and vice versa.\n\nfrom spacy.lang.en import English\n\nnlp = English()\ndoc = nlp(\"I have a cat\")\n\n# Look up the hash for the word \"cat\"\ncat_hash = doc.vocab.strings[\"cat\"]\nprint(cat_hash)\n\n# Look up the cat_hash to get the string\ncat_string = doc.vocab.strings[cat_hash]\nprint(cat_string)","execution_count":1,"outputs":[{"output_type":"stream","text":"5439657043933447811\ncat\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import spacy\n\nnlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp(\"David Bowie is a PERSON\")\n\n# Look up the hash for the string label \"PERSON\"\nperson_hash = nlp.vocab.strings[\"PERSON\"]\nprint(person_hash)\n\n# Look up the person_hash to get the string\nperson_string = nlp.vocab.strings[person_hash]\nprint(person_string)","execution_count":2,"outputs":[{"output_type":"stream","text":"380\nPERSON\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from spacy.lang.en import English\n\nnlp = English()\n\n# Import the Doc class\nfrom spacy.tokens import Doc\n\n# Desired text: \"spaCy is cool!\"\nwords = [\"spaCy\", \"is\", \"cool\", \"!\"]\nspaces = [True, True, False, False]\n\n# Create a Doc from the words and spaces\ndoc = Doc(nlp.vocab, words=words, spaces=spaces)\nprint(doc.text)","execution_count":3,"outputs":[{"output_type":"stream","text":"spaCy is cool!\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from spacy.lang.en import English\n\nnlp = English()\n\n# Import the Doc class\nfrom spacy.tokens import Doc\n\n# Desired text: \"Go, get started!\"\nwords = [\"Go\", \",\", \"get\", \"started\", \"!\"]\nspaces = [False, True, True, False, False]\n\n# Create a Doc from the words and spaces\ndoc = Doc(nlp.vocab, words=words, spaces=spaces)\nprint(doc.text)","execution_count":4,"outputs":[{"output_type":"stream","text":"Go, get started!\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from spacy.lang.en import English\n\nnlp = English()\n# Import the Doc and Span classes\nfrom spacy.tokens import Doc, Span\nwords = [\"I\", \"like\", \"David\", \"Bowie\"]\nspaces = [True, True, True, False]\n# Create a doc from the words and spaces\ndoc = Doc(nlp.vocab, words=words, spaces=spaces)\nprint(doc.text)\n# Create a span for \"David Bowie\" from the doc and assign it the label \"PERSON\"\nspan = Span(doc, 2, 4, label=\"PERSON\")\nprint(span.text, span.label_)\n# Add the span to the doc's entities\ndoc.ents = [span]\n# Print entities' text and labels\nprint([(ent.text, ent.label_) for ent in doc.ents])","execution_count":6,"outputs":[{"output_type":"stream","text":"I like David Bowie\nDavid Bowie PERSON\n[('David Bowie', 'PERSON')]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import spacy\n\nnlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp(\"Berlin is a nice city\")\n\n# Iterate over the tokens\nfor token in doc:\n    # Check if the current token is a proper noun\n    if token.pos_ == \"PROPN\":\n        # Check if the next token is a verb\n        if doc[token.i + 1].pos_ == \"VERB\":\n            print(\"Found proper noun before a verb:\", token.text)","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Word vectors and sematic similarity\n\n# for similiarity we need md or lg model\n# not sm small model\n\nnlp = spacy.load('en_core_web_lg') # Kaggle doesn't have md model\ndoc1 = nlp('I like pizza')\ndoc2 = nlp('I like mongo')\n\ndoc1.similarity(doc2)","execution_count":10,"outputs":[{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"0.7433707677370327"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tokens also have similarity method\nspan = nlp(\"I like pizza\")[1:2]\ndoc = nlp('I like fast food')\nprint(span.text)\n\nspan.similarity(doc)","execution_count":11,"outputs":[{"output_type":"stream","text":"like\n","name":"stdout"},{"output_type":"execute_result","execution_count":11,"data":{"text/plain":"0.7639933774312703"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# WordVector\ndoc[1].vector","execution_count":12,"outputs":[{"output_type":"execute_result","execution_count":12,"data":{"text/plain":"array([-1.8417e-01,  5.5115e-02, -3.6953e-01, -2.0895e-01,  2.5672e-01,\n        3.0142e-01,  1.6299e-01, -1.6437e-01, -7.0268e-02,  2.1638e+00,\n       -2.6596e-01, -5.3913e-02,  4.8184e-01, -4.2364e-02, -3.8094e-01,\n        5.3032e-02,  6.4535e-02,  7.1668e-01, -4.6026e-01, -5.7277e-02,\n        7.4186e-02,  1.2579e-01, -1.8899e-01,  1.8629e-03, -2.2974e-01,\n       -7.9151e-02, -6.1680e-02, -2.9569e-01,  3.9512e-01, -2.7898e-01,\n        1.7969e-01,  3.2117e-01, -1.2749e-01,  4.8180e-02,  2.3280e-01,\n       -2.6813e-01, -7.7054e-02,  1.4699e-01, -2.7115e-01,  9.7735e-02,\n       -1.7633e-01,  2.9249e-01, -2.0190e-01, -5.0423e-03,  8.9681e-02,\n        2.6298e-02, -4.0579e-01, -2.3846e-01, -8.9078e-02,  6.7801e-02,\n       -3.4847e-01,  1.9535e-01, -1.4056e-01,  3.5658e-02,  5.8399e-02,\n        8.7126e-03, -2.6536e-01, -3.1788e-01, -7.5300e-03, -4.7621e-02,\n       -4.5320e-02, -5.0406e-02, -1.5273e-01,  1.5345e-01,  1.0652e-01,\n       -3.1239e-01, -7.2768e-02,  1.7814e-01,  6.7195e-03,  1.6758e-01,\n        1.0098e-01, -1.7167e-02,  3.0886e-01,  1.3812e-01, -1.6494e-01,\n        1.9055e-01,  4.0531e-02, -7.6917e-02,  7.9695e-03,  7.9391e-02,\n        5.8760e-02, -2.8305e-02, -4.3925e-02, -3.1812e-01,  1.5871e-01,\n       -3.0450e-01,  1.1236e-01, -1.0799e-01,  1.2353e-01,  1.5903e-02,\n       -2.6255e-02,  9.9118e-02,  3.8579e-02,  2.6429e-01,  3.7398e-01,\n       -5.8733e-02,  2.7642e-01, -1.0507e-01, -3.5963e-02, -2.8081e-01,\n       -1.3728e-01,  1.4295e-01,  1.5272e-02, -3.0837e-01,  2.4092e-01,\n       -6.1989e-01, -2.6469e-01, -1.8974e-01,  4.0748e-02,  1.2117e-01,\n       -8.2668e-02, -4.4065e-01,  8.6471e-02, -1.2709e-01,  1.5990e-02,\n        3.3854e-02, -3.0663e-02,  8.8711e-02, -7.9608e-02,  5.9985e-02,\n       -5.1869e-02, -3.2071e-01, -1.8656e-02, -2.0263e-01,  1.0617e-01,\n        1.7099e-01,  2.5696e-01, -6.2717e-02,  1.3324e-01,  6.4167e-02,\n       -1.6250e-01,  7.3949e-02, -1.4220e-02,  1.6397e-01,  1.1049e-01,\n        1.3302e-01, -1.7611e-01, -2.8071e-01,  1.9069e-01, -1.2793e-02,\n       -2.2899e+00,  3.5692e-01,  2.3283e-01, -1.5418e-01, -1.5190e-01,\n       -2.8603e-01,  1.1693e-01,  8.2202e-02,  1.1641e-01, -2.5483e-01,\n        1.3915e-01, -7.5754e-02,  3.9335e-02,  9.1205e-02, -2.9134e-02,\n       -9.3811e-03, -3.4378e-02, -1.7371e-01,  4.7870e-02, -2.7487e-01,\n       -6.9063e-02,  7.6406e-02, -2.0904e-01,  1.7357e-02, -9.7605e-02,\n       -1.9146e-01,  1.7176e-01, -1.1660e-01,  2.0371e-01, -2.9119e-01,\n       -1.0227e-01, -6.6013e-02,  7.6949e-02, -2.1308e-01,  3.1002e-01,\n        2.3805e-02, -2.9262e-01,  2.0331e-01,  2.1265e-01, -4.1312e-01,\n        6.8592e-02, -3.2202e-02, -3.1468e-01, -3.0198e-01,  1.1675e-02,\n       -2.6810e-02, -8.1851e-02, -7.9213e-02,  1.8717e-01,  2.5725e-02,\n        2.5265e-01, -1.2247e-02, -1.4258e-01, -3.9370e-01, -4.2377e-02,\n        2.6646e-01,  6.7777e-02, -1.3359e-02, -2.1336e-01,  2.0191e-01,\n       -5.8713e-02, -3.2913e-01,  8.3644e-02, -2.9294e-01,  2.0486e-01,\n        1.4398e-03,  2.1117e-01, -2.5692e-02,  2.6377e-01, -9.5896e-03,\n       -3.8553e-01, -1.3433e-01,  1.4112e-01, -3.8692e-01,  9.5787e-02,\n        1.5890e-01, -7.5328e-02, -5.0245e-02, -1.6486e-01, -3.3025e-01,\n        2.1782e-01,  3.5559e-01, -1.3000e-01,  2.1543e-01,  2.4106e-01,\n        4.4750e-02,  4.3324e-02,  5.2225e-01, -3.2734e-03,  1.4843e-01,\n       -2.9149e-01, -3.5309e-02,  7.7393e-02, -1.3450e-01,  8.9868e-03,\n       -1.8739e-01,  7.4570e-02, -6.1989e-02,  1.3539e-01,  4.7249e-01,\n        8.6249e-03,  3.3400e-01, -8.6598e-02,  3.5765e-01,  8.1795e-02,\n       -3.4776e-02, -4.0300e-02, -9.1264e-02, -9.8390e-02,  1.7966e-01,\n        2.0194e-01, -2.4284e-01, -7.5441e-02,  2.0518e-01, -8.1308e-02,\n        3.6672e-01,  7.9708e-02,  1.6162e-01, -7.4704e-02,  1.0336e-01,\n        3.8967e-01,  2.8706e-01, -1.6609e-02,  2.4243e-01,  2.4242e-01,\n        1.3441e-01,  1.4167e-02,  1.5583e-02,  7.0343e-01,  2.9781e-01,\n       -2.5065e-02, -2.4716e-01,  7.9267e-02, -5.2218e-02, -1.8033e-01,\n        9.5872e-02,  1.5045e-02, -2.5588e-02,  1.1533e-01,  9.4677e-02,\n       -2.5210e-01, -1.0668e-01,  1.3523e-01, -1.8699e-01,  1.8749e-01,\n       -2.5540e-01,  6.7724e-01, -4.1646e-01,  1.4860e-01, -2.6401e-01,\n        2.9923e-02,  1.9484e-01, -1.3368e-01,  2.8504e-03, -3.0414e-01,\n        2.8539e-01, -3.3109e-01, -2.3808e-01,  3.7132e-01,  3.6197e-01],\n      dtype=float32)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Faster matcher PhraseMatcher \nfrom spacy.matcher import PhraseMatcher\nmatcher = PhraseMatcher(nlp.vocab)\n# do not require list of dicts\npattern = nlp('Golden Retriever')\nmatcher.add(\"Dog\",None,pattern)\n\ndoc = nlp('I have Golden Retriever')\n\nfor match_id, start, end in matcher(doc):\n    print(doc[start:end].text)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}